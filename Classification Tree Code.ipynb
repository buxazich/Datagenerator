{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, criterion='entropy'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.tree = None                                                          # переменная, в которой будет храниться готовое дерево решений.\n",
    "        self.feature_importances = None                                          # переменная для важности фич\n",
    "\n",
    "    def entropy(self, y):\n",
    "        counts = np.bincount(y)                                                   # Считаем количество объектов для каждого класса. Формат - [0,0,1,2,1,2,0]\n",
    "        probabilities = counts / len(y)                                           # вероятность. Формат - [x/y, x1/y, x3/y]\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])          # суммируем вероятности. p - каждая итерация в полученном массиве 'probabilities'.\n",
    "\n",
    "    def gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "    def information_gain(self, y, left_indices, right_indices):\n",
    "        if self.criterion == 'entropy':                                            # Выбор критерия\n",
    "            impurity_func = self.entropy\n",
    "        elif self.criterion == 'gini':\n",
    "            impurity_func = self.gini\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
    "\n",
    "        parent_impurity = impurity_func(y)                                         # неопределенность для всей выборки.\n",
    "        left_impurity = impurity_func(y[left_indices])\n",
    "        right_impurity = impurity_func(y[right_indices])\n",
    "\n",
    "        n, n_left, n_right = len(y), len(left_indices), len(right_indices)\n",
    "        weighted_impurity = (n_left / n) * left_impurity + (n_right / n) * right_impurity\n",
    "        inf_gain = parent_impurity - weighted_impurity\n",
    "        \n",
    "        # print(f'Inf. gain \"{self.criterion}\": {inf_gain}')\n",
    "        return inf_gain                                                            # возвращаем инф. выиг.\n",
    "    \n",
    "    \n",
    "    def custom_1(self, y_oh, left_indices, right_indices):\n",
    "        N = y_oh.sum()\n",
    "\n",
    "        left = y_oh[left_indices]\n",
    "        right = y_oh[right_indices]\n",
    "        p_1 = left.sum() / N\n",
    "        p_2 = right.sum() / N\n",
    "        num_classes = y_oh.shape[1]                                                # .shape[1] кол-во столбцов, .shape[0] - кол-во строк.\n",
    "\n",
    "        sum_total = 0\n",
    "        \n",
    "        for l in range(num_classes):\n",
    "            p_1l = left[:, l].sum() / N\n",
    "            p_2l = right[:, l].sum() / N\n",
    "            p_l = p_1l + p_2l\n",
    "            b = 1\n",
    "            \n",
    "            # Избегаем деления на ноль\n",
    "            # if p_1 > 0:\n",
    "            sum_total += ((p_1l - p_1 * p_l)**2) / p_1 * b**2\n",
    "            # if p_2 > 0:\n",
    "            sum_total += ((p_2l - p_2 * p_l)**2) / p_2 * b**2\n",
    "\n",
    "        return N * sum_total\n",
    "    \n",
    "    \n",
    "    def custom_2(self, y_oh, left_indices, right_indices):\n",
    "        N = y_oh.sum()\n",
    "\n",
    "        left = y_oh[left_indices]\n",
    "        right = y_oh[right_indices]\n",
    "        p_1 = left.sum() / N\n",
    "        p_2 = right.sum() / N\n",
    "        num_classes = y_oh.shape[1]                                                # .shape[1] кол-во столбцов, .shape[0] - кол-во строк.\n",
    "\n",
    "        sum_total = 0\n",
    "        \n",
    "        for l in range(num_classes):\n",
    "            p_1l = left[:, l].sum() / N\n",
    "            p_2l = right[:, l].sum() / N\n",
    "            p_l = p_1l + p_2l\n",
    "            \n",
    "            b = np.sqrt(p_l)\n",
    "            \n",
    "            # Избегаем деления на ноль\n",
    "            # if p_1 > 0:\n",
    "            sum_total += ((p_1l - p_1 * p_l)**2) / p_1 * b**2\n",
    "            # if p_2 > 0:\n",
    "            sum_total += ((p_2l - p_2 * p_l)**2) / p_2 * b**2\n",
    "\n",
    "        return N * sum_total\n",
    "    \n",
    "\n",
    "    def custom_3(self, y_oh, left_indices, right_indices):\n",
    "        N = y_oh.sum()\n",
    "\n",
    "        left = y_oh[left_indices]\n",
    "        right = y_oh[right_indices]\n",
    "        p_1 = left.sum() / N\n",
    "        p_2 = right.sum() / N\n",
    "        num_classes = y_oh.shape[1]                                                # .shape[1] кол-во столбцов, .shape[0] - кол-во строк.\n",
    "\n",
    "        sum_total = 0\n",
    "        epsilon = 1e-10 \n",
    "        \n",
    "        for l in range(num_classes):\n",
    "            p_1l = left[:, l].sum() / N\n",
    "            p_2l = right[:, l].sum() / N\n",
    "            p_l = p_1l + p_2l\n",
    "            \n",
    "            b = np.sqrt(p_l*(1 - p_l))\n",
    "            \n",
    "            # eps. для стабильности вычислений\n",
    "            denominator_1 = max(p_1 * b**2, epsilon)\n",
    "            denominator_2 = max(p_2 * b**2, epsilon)\n",
    "            \n",
    "            sum_total += ((p_1l - p_1 * p_l)**2) / denominator_1\n",
    "            sum_total += ((p_2l - p_2 * p_l)**2) / denominator_2\n",
    "\n",
    "        return N * sum_total\n",
    "    \n",
    "\n",
    "    def custom_4(self, y_oh, left_indices, right_indices):\n",
    "        N = y_oh.sum()\n",
    "\n",
    "        left = y_oh[left_indices]\n",
    "        right = y_oh[right_indices]\n",
    "        p_1 = left.sum() / N\n",
    "        p_2 = right.sum() / N\n",
    "        num_classes = y_oh.shape[1]                                                # .shape[1] кол-во столбцов, .shape[0] - кол-во строк.\n",
    "\n",
    "        sum_total = 0\n",
    "        \n",
    "        for l in range(num_classes):\n",
    "            p_1l = left[:, l].sum() / N\n",
    "            p_2l = right[:, l].sum() / N\n",
    "            p_l = p_1l + p_2l\n",
    "            \n",
    "            b = p_l\n",
    "            \n",
    "            sum_total += ((p_1l - p_1 * p_l)**2) / p_1 * b**2\n",
    "            sum_total += ((p_2l - p_2 * p_l)**2) / p_2 * b**2\n",
    "\n",
    "\n",
    "        return N * sum_total\n",
    "    \n",
    "    \n",
    "    def custom_5(self, y_oh, left_indices, right_indices):\n",
    "        N = y_oh.sum()\n",
    "\n",
    "        left = y_oh[left_indices]\n",
    "        right = y_oh[right_indices]\n",
    "        p_1 = left.sum() / N\n",
    "        p_2 = right.sum() / N\n",
    "        num_classes = y_oh.shape[1]                                                # .shape[1] кол-во столбцов, .shape[0] - кол-во строк.\n",
    "\n",
    "        sum_total = 0\n",
    "        \n",
    "        for l in range(num_classes):\n",
    "            p_1l = left[:, l].sum() / N\n",
    "            p_2l = right[:, l].sum() / N\n",
    "            p_l = p_1l + p_2l\n",
    "            \n",
    "            b = p_l**2\n",
    "            \n",
    "            sum_total += ((p_1l - p_1 * p_l)**2) / p_1 * b**2\n",
    "            sum_total += ((p_2l - p_2 * p_l)**2) / p_2 * b**2\n",
    "\n",
    "        return N * sum_total\n",
    "    \n",
    "\n",
    "    def custom_6(self, y_oh, left_indices, right_indices):\n",
    "        N = y_oh.sum()\n",
    "\n",
    "        left = y_oh[left_indices]\n",
    "        right = y_oh[right_indices]\n",
    "        p_1 = left.sum() / N\n",
    "        p_2 = right.sum() / N\n",
    "        num_classes = y_oh.shape[1]                                                # .shape[1] кол-во столбцов, .shape[0] - кол-во строк.\n",
    "\n",
    "        sum_total = 0\n",
    "        epsilon = 1e-10\n",
    "        \n",
    "        for l in range(num_classes):\n",
    "            p_1l = left[:, l].sum() / N\n",
    "            p_2l = right[:, l].sum() / N\n",
    "            p_l = p_1l + p_2l\n",
    "            \n",
    "            b = -np.log(max(p_l, epsilon))\n",
    "            \n",
    "            sum_total += ((p_1l - p_1 * p_l)**2) / p_1 * b**2\n",
    "            sum_total += ((p_2l - p_2 * p_l)**2) / p_2 * b**2\n",
    "\n",
    "        return N * sum_total\n",
    "    \n",
    "\n",
    "    def custom_7(self, y_oh, left_indices, right_indices):\n",
    "        N = y_oh.sum()\n",
    "\n",
    "        left = y_oh[left_indices]\n",
    "        right = y_oh[right_indices]\n",
    "        p_1 = left.sum() / N\n",
    "        p_2 = right.sum() / N\n",
    "        num_classes = y_oh.shape[1]                                                # .shape[1] кол-во столбцов, .shape[0] - кол-во строк.\n",
    "\n",
    "        sum_total = 0\n",
    "        epsilon = 1e-10\n",
    "        \n",
    "        for l in range(num_classes):\n",
    "            p_1l = left[:, l].sum() / N\n",
    "            p_2l = right[:, l].sum() / N\n",
    "            p_l = p_1l + p_2l\n",
    "            \n",
    "            b = (-p_l)*np.log(max(p_l, epsilon))\n",
    "            \n",
    "            # # eps. для стабильности вычислений\n",
    "            # denominator_1 = max(p_1 * b**2, epsilon)\n",
    "            # denominator_2 = max(p_2 * b**2, epsilon)\n",
    "            \n",
    "            sum_total += ((p_1l - p_1 * p_l)**2) / p_1 * b**2\n",
    "            sum_total += ((p_2l - p_2 * p_l)**2) / p_2 * b**2\n",
    "\n",
    "        return N * sum_total\n",
    "    \n",
    "    \n",
    "    def custom_8(self, y_oh, left_indices, right_indices):\n",
    "        N = y_oh.sum()\n",
    "\n",
    "        left = y_oh[left_indices]\n",
    "        right = y_oh[right_indices]\n",
    "        p_1 = left.sum() / N\n",
    "        p_2 = right.sum() / N\n",
    "        num_classes = y_oh.shape[1]                                                # .shape[1] кол-во столбцов, .shape[0] - кол-во строк.\n",
    "\n",
    "        sum_total = 0\n",
    "        epsilon = 1e-10\n",
    "        \n",
    "        for l in range(num_classes):\n",
    "            p_1l = left[:, l].sum() / N\n",
    "            p_2l = right[:, l].sum() / N\n",
    "            p_l = p_1l + p_2l\n",
    "            \n",
    "            b = -(p_l**0.5) * np.log(max(p_l, epsilon))\n",
    "            \n",
    "            # # eps. для стабильности вычислений\n",
    "            # denominator_1 = max(p_1 * b**2, epsilon)\n",
    "            # denominator_2 = max(p_2 * b**2, epsilon)\n",
    "            \n",
    "            sum_total += ((p_1l - p_1 * p_l)**2) / p_1 * b**2\n",
    "            sum_total += ((p_2l - p_2 * p_l)**2) / p_2 * b**2\n",
    "\n",
    "        return N * sum_total    \n",
    "    \n",
    "    # Функция находит наиболее частый элемент в массиве y (метки классов).\n",
    "    def most_common_label(self, y):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "    def find_best_split(self, X, y, num_features, y_oh=None):\n",
    "        best_gain = -float('inf')                                                  # хранит лучшее значение критерия\n",
    "        best_split = None                                                          # будет содержать параметры наилучшего разбиения\n",
    "\n",
    "        for feature_index in range(num_features):                                  # перебираем по очереди признаки.\n",
    "            # Сортируем значения признака\n",
    "            feature_values = np.sort(X[:, feature_index])\n",
    "            # Берем средние между соседними значениями\n",
    "            thresholds = (feature_values[:-1] + feature_values[1:]) / 2     \n",
    "            \n",
    "            for threshold in thresholds:                                           # для каждого уникального значения делим данные на 2 части.\n",
    "                left_indices = np.where(X[:, feature_index] <= threshold)[0]       # левый - меньше уникального значения. [0] - нужен для возвращения массива, а не кортежа.\n",
    "                right_indices = np.where(X[:, feature_index] > threshold)[0]       # правый - больше ун. знач. feature_index - искомый признак.\n",
    "\n",
    "                if (len(left_indices) < self.min_samples_leaf or \n",
    "                    len(right_indices) < self.min_samples_leaf):\n",
    "                    continue                                                       # если условие срабатывает, переходим к следующей итерации, пропуская то, что ниже.\n",
    "\n",
    "                if self.criterion == 'custom_1':\n",
    "                    if y_oh is None:\n",
    "                        raise ValueError(\"y_oh required for custom_1 criterion\")\n",
    "                    gain = self.custom_1(y_oh, left_indices, right_indices)\n",
    "                \n",
    "                elif self.criterion == 'custom_2':\n",
    "                    if y_oh is None:\n",
    "                        raise ValueError(\"y_oh required for custom_2 criterion\")\n",
    "                    gain = self.custom_2(y_oh, left_indices, right_indices)\n",
    "                \n",
    "                elif self.criterion == 'custom_3':\n",
    "                    if y_oh is None:\n",
    "                        raise ValueError(\"y_oh required for custom_3 criterion\")\n",
    "                    gain = self.custom_3(y_oh, left_indices, right_indices)                    \n",
    "                \n",
    "                elif self.criterion == 'custom_4':\n",
    "                    if y_oh is None:\n",
    "                        raise ValueError(\"y_oh required for custom_4 criterion\")\n",
    "                    gain = self.custom_4(y_oh, left_indices, right_indices)\n",
    "                \n",
    "                elif self.criterion == 'custom_5':\n",
    "                    if y_oh is None:\n",
    "                        raise ValueError(\"y_oh required for custom_5 criterion\")\n",
    "                    gain = self.custom_5(y_oh, left_indices, right_indices)\n",
    "                \n",
    "                elif self.criterion == 'custom_6':\n",
    "                    if y_oh is None:\n",
    "                        raise ValueError(\"y_oh required for custom_6 criterion\")\n",
    "                    gain = self.custom_6(y_oh, left_indices, right_indices)    \n",
    "                    \n",
    "                elif self.criterion == 'custom_7':\n",
    "                    if y_oh is None:\n",
    "                        raise ValueError(\"y_oh required for custom_7 criterion\")\n",
    "                    gain = self.custom_7(y_oh, left_indices, right_indices)\n",
    "                    \n",
    "                elif self.criterion == 'custom_8':\n",
    "                    if y_oh is None:\n",
    "                        raise ValueError(\"y_oh required for custom_7 criterion\")\n",
    "                    gain = self.custom_8(y_oh, left_indices, right_indices)                      \n",
    "                \n",
    "                else:\n",
    "                    gain = self.information_gain(y, left_indices, right_indices)   # рассчитываем инф. прирост.\n",
    "\n",
    "                if gain > best_gain:                                               # если текущий прирост больше самого большого\n",
    "                    best_gain = gain                                               # приравниваем переменную наибольшего к текущему.\n",
    "                    best_split = {\n",
    "                        'feature_index': feature_index,\n",
    "                        'threshold': threshold,\n",
    "                        'left_indices': left_indices,\n",
    "                        'right_indices': right_indices,\n",
    "                        'gain': gain\n",
    "                    }                                                              # теперь это параметры разбиения, которые дают наилучший прирост.\n",
    "        \n",
    "        return best_split                                                          # После перебора всех признаков и порогов, возвращаем параметры лучшего найденного разбиения.\n",
    "\n",
    "\n",
    "    def fit(self, X, y, y_oh=None):\n",
    "        num_features = X.shape[1]\n",
    "        self.feature_importances = np.zeros(num_features)                          # инициализируем нулями\n",
    "        self.tree = self.grow_tree(X, y, y_oh, depth=0)\n",
    "\n",
    "        # нормализуем важности, чтобы сумма = 1, как в sklearn\n",
    "        total = self.feature_importances.sum()\n",
    "        if total > 0:\n",
    "            self.feature_importances /= total\n",
    "\n",
    "\n",
    "    def grow_tree(self, X, y, y_oh, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_classes = len(set(y))\n",
    "\n",
    "        if (depth == self.max_depth or \n",
    "            num_classes == 1 or \n",
    "            num_samples < self.min_samples_split):\n",
    "            return self.most_common_label(y)\n",
    "\n",
    "        if self.criterion.startswith('custom_'):\n",
    "            best_split = self.find_best_split(X, y, num_features, y_oh)\n",
    "        else:\n",
    "            best_split = self.find_best_split(X, y, num_features)\n",
    "\n",
    "        if best_split is None:\n",
    "            return self.most_common_label(y)\n",
    "\n",
    "        left_indices, right_indices = best_split['left_indices'], best_split['right_indices']\n",
    "        \n",
    "        # Вычисляем прирост информации для подсчета важности признаков\n",
    "        if self.criterion == 'custom_1':\n",
    "            gain = self.custom_1(y_oh, left_indices, right_indices)\n",
    "        elif self.criterion == 'custom_2':\n",
    "            gain = self.custom_2(y_oh, left_indices, right_indices)\n",
    "        elif self.criterion == 'custom_3':\n",
    "            gain = self.custom_3(y_oh, left_indices, right_indices)\n",
    "        elif self.criterion == 'custom_4':\n",
    "            gain = self.custom_4(y_oh, left_indices, right_indices)\n",
    "        elif self.criterion == 'custom_5':\n",
    "            gain = self.custom_5(y_oh, left_indices, right_indices)\n",
    "        elif self.criterion == 'custom_6':\n",
    "            gain = self.custom_6(y_oh, left_indices, right_indices)\n",
    "        elif self.criterion == 'custom_7':\n",
    "            gain = self.custom_7(y_oh, left_indices, right_indices)\n",
    "        elif self.criterion == 'custom_8':\n",
    "            gain = self.custom_8(y_oh, left_indices, right_indices)            \n",
    "            \n",
    "        else:\n",
    "            gain = self.information_gain(y, left_indices, right_indices)\n",
    "\n",
    "        self.feature_importances[best_split['feature_index']] += gain              # Сохраняем вклад этого признака в важность\n",
    "\n",
    "        left_subtree = self.grow_tree(X[left_indices], y[left_indices], \n",
    "                                    y_oh[left_indices] if y_oh is not None else None, \n",
    "                                    depth + 1)\n",
    "        right_subtree = self.grow_tree(X[right_indices], y[right_indices], \n",
    "                                     y_oh[right_indices] if y_oh is not None else None, \n",
    "                                     depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature_index': best_split['feature_index'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if isinstance(node, dict):\n",
    "            if x[node['feature_index']] <= node['threshold']:\n",
    "                return self._traverse_tree(x, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(x, node['right'])\n",
    "\n",
    "        return node                                                             # Если нет, то это лист и присваиваем метку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def compare_metrics_train_test(max_depth, X, y, *, N=None, V=None, k=None, alpha=None, nmin=None, random_state=42):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state)\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_oh_train = encoder.fit_transform(y_train.reshape(-1,1))\n",
    "\n",
    "\n",
    "    '''Custom_1'''\n",
    "    custom_1 = DecisionTree(max_depth=max_depth, criterion='custom_1')\n",
    "    custom_1.fit(X_train, y_train, y_oh_train)\n",
    "    y_pred = custom_1.predict(X_test)\n",
    "    accuracy_1, precision_1 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_1, f1_1 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_1 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''GINI'''\n",
    "    gini = DecisionTree(max_depth=max_depth, criterion='gini')\n",
    "    gini.fit(X_train, y_train)\n",
    "    y_pred = gini.predict(X_test)\n",
    "    accuracy_gini, precision_gini = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_gini, f1_gini = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_gini = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''Sklearn_GINI'''\n",
    "    sk_gini = DecisionTreeClassifier(max_depth=max_depth, criterion='gini')\n",
    "    sk_gini.fit(X_train, y_train)\n",
    "    y_pred = sk_gini.predict(X_test)\n",
    "    accuracy_gini_sk, precision_gini_sk = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_gini_sk, f1_gini_sk = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_gini_sk = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''Entropy'''\n",
    "    entropy = DecisionTree(max_depth=max_depth, criterion='entropy')\n",
    "    entropy.fit(X_train, y_train)\n",
    "    y_pred = entropy.predict(X_test)\n",
    "    accuracy_entropy, precision_entropy = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_entropy, f1_entropy = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_entropy = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''Sklearn_Entropy'''\n",
    "    sk_entropy = DecisionTreeClassifier(max_depth=max_depth, criterion='entropy')\n",
    "    sk_entropy.fit(X_train, y_train)\n",
    "    y_pred = sk_entropy.predict(X_test)\n",
    "    accuracy_entropy_sk, precision_entropy_sk = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_entropy_sk, f1_entropy_sk = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_entropy_sk = adjusted_rand_score(y_test, y_pred)\n",
    "    \n",
    "    '''Custom_2'''\n",
    "    custom_2 = DecisionTree(max_depth=max_depth, criterion='custom_2')\n",
    "    custom_2.fit(X_train, y_train, y_oh_train)\n",
    "    y_pred = custom_2.predict(X_test)\n",
    "    accuracy_2, precision_2 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_2, f1_2 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_2 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''Custom_3'''\n",
    "    custom_3 = DecisionTree(max_depth=max_depth, criterion='custom_3')\n",
    "    custom_3.fit(X_train, y_train, y_oh_train)\n",
    "    y_pred = custom_3.predict(X_test)\n",
    "    accuracy_3, precision_3 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_3, f1_3 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_3 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''Custom_4'''\n",
    "    custom_4 = DecisionTree(max_depth=max_depth, criterion='custom_4')\n",
    "    custom_4.fit(X_train, y_train, y_oh_train)\n",
    "    y_pred = custom_4.predict(X_test)\n",
    "    accuracy_4, precision_4 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_4, f1_4 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_4 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''Custom_5'''\n",
    "    custom_5 = DecisionTree(max_depth=max_depth, criterion='custom_5')\n",
    "    custom_5.fit(X_train, y_train, y_oh_train)\n",
    "    y_pred = custom_5.predict(X_test)\n",
    "    accuracy_5, precision_5 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_5, f1_5 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_5 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''Custom_6'''\n",
    "    custom_6 = DecisionTree(max_depth=max_depth, criterion='custom_6')\n",
    "    custom_6.fit(X_train, y_train, y_oh_train)\n",
    "    y_pred = custom_6.predict(X_test)\n",
    "    accuracy_6, precision_6 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_6, f1_6 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_6 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    '''Custom_7'''\n",
    "    custom_7 = DecisionTree(max_depth=max_depth, criterion='custom_7')\n",
    "    custom_7.fit(X_train, y_train, y_oh_train)\n",
    "    y_pred = custom_7.predict(X_test)\n",
    "    accuracy_7, precision_7 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_7, f1_7 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_7 = adjusted_rand_score(y_test, y_pred)\n",
    "    \n",
    "    '''Custom_8'''\n",
    "    custom_8 = DecisionTree(max_depth=max_depth, criterion='custom_8')\n",
    "    custom_8.fit(X_train, y_train, y_oh_train)\n",
    "    y_pred = custom_8.predict(X_test)\n",
    "    accuracy_8, precision_8 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall_8, f1_8 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "    ari_8 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "    results = np.round([[accuracy_1, accuracy_gini, accuracy_gini_sk, accuracy_entropy, accuracy_entropy_sk, accuracy_2, accuracy_3, accuracy_4, accuracy_5, accuracy_6, accuracy_7, accuracy_8],\n",
    "                    [precision_1, precision_gini, precision_gini_sk, precision_entropy, precision_entropy_sk, precision_2, precision_3, precision_4, precision_5, precision_6, precision_7, precision_8],\n",
    "                    [recall_1, recall_gini, recall_gini_sk, recall_entropy, recall_entropy_sk, recall_2, recall_3, recall_4, recall_5, recall_6, recall_7, recall_8],\n",
    "                    [f1_1, f1_gini, f1_gini_sk, f1_entropy, f1_entropy_sk, f1_2, f1_3, f1_4, f1_5, f1_6, f1_7, f1_8],\n",
    "                    [ari_1, ari_gini, ari_gini_sk, ari_entropy, ari_entropy_sk, ari_2, ari_3, ari_4, ari_5, ari_6, ari_7, ari_8],],4)\n",
    "\n",
    "    column = ['b = 1','gini','gini_sklearn', 'entropy', 'entropy_sklearn', 'b = p_l ^ 0.5', 'b = (p_l*(1 - p_l)) ^ 0.5', 'b = p_l', 'b = p_l ^ 2', 'b = log(p_l)', 'b = -p_l * log(p_l)', 'b = p_l^0.5 * log(p_l)']\n",
    "    table = pd.DataFrame(data=results, columns=column, index=['Accuracy', 'Precision', 'Recall','F1 score','ARI'])\n",
    "    \n",
    "    print(f'\\nN, V, k, alpha, nmin, max_depth = {N, V, k, alpha, nmin, max_depth}')\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean/std of 50 exps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def compare_metrics_train_test(max_depth, X, y, *, N=None, V=None, k=None, alpha=None, nmin=None):\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for seed in range(1,51):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        y_oh_train = encoder.fit_transform(y_train.reshape(-1,1))\n",
    "\n",
    "\n",
    "        '''Custom_1'''\n",
    "        custom_1 = DecisionTree(max_depth=max_depth, criterion='custom_1')\n",
    "        custom_1.fit(X_train, y_train, y_oh_train)\n",
    "        y_pred = custom_1.predict(X_test)\n",
    "        accuracy_1, precision_1 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_1, f1_1 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_1 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        # '''GINI'''\n",
    "        # gini = DecisionTree(max_depth=max_depth, criterion='gini')\n",
    "        # gini.fit(X_train, y_train)\n",
    "        # y_pred = gini.predict(X_test)\n",
    "        # accuracy_gini, precision_gini = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        # recall_gini, f1_gini = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        # ari_gini = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        # '''Sklearn_GINI'''\n",
    "        # sk_gini = DecisionTreeClassifier(max_depth=max_depth, criterion='gini')\n",
    "        # sk_gini.fit(X_train, y_train)\n",
    "        # y_pred = sk_gini.predict(X_test)\n",
    "        # accuracy_gini_sk, precision_gini_sk = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        # recall_gini_sk, f1_gini_sk = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        # ari_gini_sk = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        # '''Entropy'''\n",
    "        # entropy = DecisionTree(max_depth=max_depth, criterion='entropy')\n",
    "        # entropy.fit(X_train, y_train)\n",
    "        # y_pred = entropy.predict(X_test)\n",
    "        # accuracy_entropy, precision_entropy = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        # recall_entropy, f1_entropy = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        # ari_entropy = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        '''Sklearn_Entropy'''\n",
    "        sk_entropy = DecisionTreeClassifier(max_depth=max_depth, criterion='entropy')\n",
    "        sk_entropy.fit(X_train, y_train)\n",
    "        y_pred = sk_entropy.predict(X_test)\n",
    "        accuracy_entropy_sk, precision_entropy_sk = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_entropy_sk, f1_entropy_sk = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_entropy_sk = adjusted_rand_score(y_test, y_pred)\n",
    "        \n",
    "        '''Custom_2'''\n",
    "        custom_2 = DecisionTree(max_depth=max_depth, criterion='custom_2')\n",
    "        custom_2.fit(X_train, y_train, y_oh_train)\n",
    "        y_pred = custom_2.predict(X_test)\n",
    "        accuracy_2, precision_2 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_2, f1_2 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_2 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        '''Custom_3'''\n",
    "        custom_3 = DecisionTree(max_depth=max_depth, criterion='custom_3')\n",
    "        custom_3.fit(X_train, y_train, y_oh_train)\n",
    "        y_pred = custom_3.predict(X_test)\n",
    "        accuracy_3, precision_3 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_3, f1_3 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_3 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        '''Custom_4'''\n",
    "        custom_4 = DecisionTree(max_depth=max_depth, criterion='custom_4')\n",
    "        custom_4.fit(X_train, y_train, y_oh_train)\n",
    "        y_pred = custom_4.predict(X_test)\n",
    "        accuracy_4, precision_4 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_4, f1_4 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_4 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        '''Custom_5'''\n",
    "        custom_5 = DecisionTree(max_depth=max_depth, criterion='custom_5')\n",
    "        custom_5.fit(X_train, y_train, y_oh_train)\n",
    "        y_pred = custom_5.predict(X_test)\n",
    "        accuracy_5, precision_5 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_5, f1_5 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_5 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        '''Custom_6'''\n",
    "        custom_6 = DecisionTree(max_depth=max_depth, criterion='custom_6')\n",
    "        custom_6.fit(X_train, y_train, y_oh_train)\n",
    "        y_pred = custom_6.predict(X_test)\n",
    "        accuracy_6, precision_6 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_6, f1_6 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_6 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        '''Custom_7'''\n",
    "        custom_7 = DecisionTree(max_depth=max_depth, criterion='custom_7')\n",
    "        custom_7.fit(X_train, y_train, y_oh_train)\n",
    "        y_pred = custom_7.predict(X_test)\n",
    "        accuracy_7, precision_7 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_7, f1_7 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_7 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        '''Custom_8'''\n",
    "        custom_8 = DecisionTree(max_depth=max_depth, criterion='custom_8')\n",
    "        custom_8.fit(X_train, y_train, y_oh_train)\n",
    "        y_pred = custom_8.predict(X_test)\n",
    "        accuracy_8, precision_8 = accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall_8, f1_8 = recall_score(y_test, y_pred, average='weighted'), f1_score(y_test, y_pred, average='weighted')\n",
    "        ari_8 = adjusted_rand_score(y_test, y_pred)\n",
    "\n",
    "        results = np.round([[accuracy_1, accuracy_entropy_sk, accuracy_2, accuracy_3, accuracy_4, accuracy_5, accuracy_6, accuracy_7, accuracy_8],\n",
    "                        [precision_1, precision_entropy_sk, precision_2, precision_3, precision_4, precision_5, precision_6, precision_7, precision_8],\n",
    "                        [recall_1, recall_entropy_sk, recall_2, recall_3, recall_4, recall_5, recall_6, recall_7, recall_8],\n",
    "                        [f1_1, f1_entropy_sk, f1_2, f1_3, f1_4, f1_5, f1_6, f1_7, f1_8],\n",
    "                        [ari_1, ari_entropy_sk, ari_2, ari_3, ari_4, ari_5, ari_6, ari_7, ari_8],],4)\n",
    "        \n",
    "        all_results.append(results)\n",
    "        print(f'Finished: {seed} iter.')\n",
    "        \n",
    "    print(f'\\nN, V, k, alpha, nmin, max_depth = {N, V, k, alpha, nmin, max_depth}')\n",
    "            \n",
    "    all_results = np.array(all_results)  # shape: (4, 5, 11)\n",
    "\n",
    "    mean_results = np.round(np.mean(all_results, axis=0),4)\n",
    "    std_results = np.round(np.std(all_results, axis=0),4)\n",
    "    \n",
    "    # Final table (Mean/std)\n",
    "    columns = ['b = 1','entropy_sklearn', 'b = p_l ^ 0.5', 'b = (p_l*(1 - p_l)) ^ 0.5', 'b = p_l', 'b = p_l ^ 2', 'b = -log(p_l)', 'b = -p_l * log(p_l)', 'b = -p_l^0.5 * log(p_l)']\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 score', 'ARI']\n",
    "    \n",
    "    index_tuples = []\n",
    "    for metric in metrics:\n",
    "        index_tuples.append((metric, 'Mean'))\n",
    "        index_tuples.append((metric, 'Std'))\n",
    "    \n",
    "    multi_index = pd.MultiIndex.from_tuples(index_tuples, names=['Metric', 'Statistic'])\n",
    "    \n",
    "    # Final table\n",
    "    final_table_data = []\n",
    "    for i in range(len(metrics)):\n",
    "        final_table_data.append(mean_results[i])\n",
    "        final_table_data.append(std_results[i])\n",
    "    \n",
    "    final_table = pd.DataFrame(final_table_data, \n",
    "                             columns=columns, \n",
    "                             index=multi_index)\n",
    "    \n",
    "    return final_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
